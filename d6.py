# -*- coding: utf-8 -*-
"""d6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxQFEB-ufZfEhrN045zaMzTcDGrlEmF4
"""

!pip install -q statsmodels optuna

# ===================== BASIC IMPORTS =====================
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.statespace.sarimax import SARIMAX
import optuna
import warnings
warnings.filterwarnings("ignore")

# ===================== DATA GENERATION =====================
np.random.seed(42)
N = 12000
t = np.arange(N)

x1 = 0.0003*t + np.sin(2*np.pi*t/40) + np.random.normal(0,0.3,N)
x2 = np.sin(2*np.pi*t/25)
x3 = np.cos(2*np.pi*t/60)
x4 = np.random.normal(0,1,N)
x5 = np.random.normal(0,1,N)

y = 0.5*x1 + 0.3*x2 + 0.2*x3 + np.random.normal(0,0.2,N)

df = pd.DataFrame({
    "x1":x1,"x2":x2,"x3":x3,"x4":x4,"x5":x5,"y":y
})

train_df = df.iloc[:10000]
test_df  = df.iloc[10000:]

# ===================== SARIMAX BASELINE =====================
sarimax = SARIMAX(
    train_df["y"],
    exog=train_df[["x1","x2","x3","x4","x5"]],
    order=(1,1,1)
)

sarimax_fit = sarimax.fit(disp=False)

sarimax_preds = sarimax_fit.predict(
    start=len(train_df),
    end=len(df)-1,
    exog=test_df[["x1","x2","x3","x4","x5"]]
)

sarimax_rmse = np.sqrt(mean_squared_error(test_df["y"], sarimax_preds))
print("SARIMAX RMSE:", sarimax_rmse)

# ===================== LSTM DATA =====================
scaler = MinMaxScaler()
X = scaler.fit_transform(df[["x1","x2","x3","x4","x5"]])
y_all = df["y"].values

X_train = torch.tensor(X[:10000]).float().unsqueeze(1)
y_train = torch.tensor(y_all[:10000]).float()
X_test  = torch.tensor(X[10000:]).float().unsqueeze(1)
y_test  = y_all[10000:]

train_loader = DataLoader(
    TensorDataset(X_train, y_train),
    batch_size=64,
    shuffle=True
)

# ===================== ATTENTION LSTM =====================
class AttentionLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.attn = nn.Linear(hidden_dim, 1)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out,_ = self.lstm(x)
        weights = torch.softmax(self.attn(out), dim=1)
        context = torch.sum(weights * out, dim=1)
        return self.fc(context), weights

# ===================== OPTUNA =====================
def objective(trial):
    hidden = trial.suggest_int("hidden", 32, 64)
    lr = trial.suggest_float("lr", 1e-3, 1e-2)

    model = AttentionLSTM(5, hidden)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    for _ in range(2):
        for xb, yb in train_loader:
            optimizer.zero_grad()
            pred,_ = model(xb)
            loss = loss_fn(pred.squeeze(), yb)
            loss.backward()
            optimizer.step()

    return loss.item()

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=5)

best_hidden = study.best_params["hidden"]
best_lr = study.best_params["lr"]

print("Best params:", study.best_params)

# ===================== FINAL TRAIN =====================
model = AttentionLSTM(5, best_hidden)
optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)
loss_fn = nn.MSELoss()

for epoch in range(5):
    for xb, yb in train_loader:
        optimizer.zero_grad()
        pred,_ = model(xb)
        loss = loss_fn(pred.squeeze(), yb)
        loss.backward()
        optimizer.step()

# ===================== EVALUATION =====================
model.eval()
with torch.no_grad():
    preds, attn = model(X_test)

preds = preds.squeeze().numpy()

mae = mean_absolute_error(y_test, preds)
rmse = np.sqrt(mean_squared_error(y_test, preds))
worst_idx = np.argsort(np.abs(y_test - preds))[-5:]

print("Attention-LSTM MAE:", mae)
print("Attention-LSTM RMSE:", rmse)
print("Top-5 Worst Prediction Indices:", worst_idx)

import matplotlib.pyplot as plt

plt.figure(figsize=(10,4))
plt.plot(y_test[:200], label="Actual", linewidth=2)
plt.plot(preds[:200], label="Predicted", linestyle="--")
plt.title("Actual vs Predicted Values (First 200 Test Points)")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.show()

# Average attention weights
avg_attn = attn.mean(dim=0).squeeze().numpy()

plt.figure(figsize=(6,4))
plt.bar([0], [avg_attn]) # Fixed: Pass a list for x and y to plt.bar
plt.title("Average Temporal Attention Weights")
plt.xlabel("Time Step")
plt.ylabel("Attention Weight")
plt.grid(True)
plt.show()